
model:
  name: "gpt2-small"
  hidden_size: 768
  num_attention_heads: 12

training:
  batch_size: 32
  learning_rate: 0.0001
  max_epochs: 10
  gradient_clip_val: 1.0

data:
  dataset_name: "wikitext"
  subset: "wikitext-2-raw-v1"
